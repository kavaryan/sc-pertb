---
title: "Data Exploration"
author: "Amir Asiaee"
date: '`r format(Sys.Date(), "%d %B, %Y")`' # Formats the date
output: 
  html_document:
    toc: true # Adds a table of contents
    toc_float: true # Floats the table of contents
    toc_depth: 2 # Sets depth of headers to include in TOC
    number_sections: true # Numbers the sections
    highlight: kate # Syntax highlighting style
    theme: yeti # Bootswatch theme
    fig_width: 10 # Width of figures in inches
    fig_height: 6 # Height of figures in inches
    css: styles.css # Link to an external CSS file
---

# Conclusion
Here is our **naming convention**:

- Combination of each cell type and drug will give us an **experiment**, i.e., each experiment is a pair of (cell type, drug) that determine a point on the given heatmap. In each experiment we have many cells measured that are our **samples**. Samples come from three doners. 
- We have 876 = (144 + 2) * 6 experiment
- Seven of them are missing and 614 of them are given as training. The rest (255) are test and we need to predict the (average) gene expression profile for each one of them. 
- The features for batch effect removal are: library (i.e., row, from 0 to 47), plate (from 0 to 5), and doner (from 0 to 2).

# Exploring Data

The file type is `parquet` and it seems `arrow` package can read it: 

```{r paths}
rm(list = ls())
source("00-paths.R")
```

## Loading X

```{r , warning = FALSE}
library(arrow)
train <- read_parquet(file.path(paths$raw, 'adata_train.parquet'))

dim(train)
colnames(train)
train[1:3, ]

length(unique(train$obs_id))
```

Looking into the meta data available here are what we concluded: 

- `obs_id` is the cell identifier
- The number of genes measured per cell can be much less than 20k. 




Now taking a look at the meta data:

```{r , warning = FALSE}
train_meta <- read.csv(file.path(paths$raw, 'adata_obs_meta.csv'))

dim(train_meta)
train_meta[1:3,]
unique(train_meta$sm_lincs_id[train_meta$sm_name == 'Dimethyl Sulfoxide']) #DMSO-negative control
```

So for each cell (`obs_id`) we have everything to run Limma. 


## Loading Y

Below is the given DE outcome for the training set: (our desired outcome $Y$)

```{r , warning = FALSE}
de_train <- read_parquet(file.path(paths$raw, 'de_train.parquet'))
dim(de_train)
de_train[1:3, 1:10]
```

## Loading Z

```{r , warning = FALSE}
library(arrow)
multiome_train <- read_parquet(file.path(paths$raw, 'multiome_train.parquet'))
dim(multiome_train)
multiome_train[1:5,]
```



```{r , warning = FALSE}
multiome_obs_meta <- read.csv(file.path(paths$raw, 'multiome_obs_meta.csv'))
multiome_var_meta <- read.csv(file.path(paths$raw, 'multiome_var_meta.csv'))

multiome_obs_meta[1:3, ]
multiome_var_meta[100:105, ]
multiome_var_meta[multiome_var_meta$feature_type == 'Peaks', ]
```
`GL000194.1 refers to a specific alternative or unplaced genomic scaffold or contig. These are often sequences that are part of the genome but haven't been precisely located on a specific chromosome or are not part of the main chromosomal assembly. They are typically labeled with "GL" or "KI" followed by a series of numbers and potentially a version number (as indicated by the ".1").
100992-101876 indicates the interval on this scaffold, from base position 100,992 to base position 101,876.

Since the whole point of peaks is that they help us figuring out which gene is close to the open region, these unknown contigs are useless and we can drop them. But it seems there are not that many of them:

```{r , warning = FALSE}
V <- multiome_var_meta$gene_id[multiome_var_meta$feature_type == 'Peaks']
count_start_with_GL_KI <- length(grep("^(GL|KI)", V, ignore.case = TRUE))
print(count_start_with_GL_KI)
```

Check some simple statistics:

```{r , warning = FALSE}
index <- multiome_train$obs_id == '000225c1151ab841'
count <- multiome_train$count[index]
hist(log2(count), breaks = 123)
measured_features <- multiome_train$location[index]
length(measured_features)
open_region <- grepl("^chr", measured_features, ignore.case = TRUE)
sum(open_region)
hist(log2(multiome_train$count[open_region]), breaks = 123)
```


## Mapping for Submission
Let's look at other files to find what we need:
```{r , warning = FALSE}
validation_id_map <- read.csv(file.path(paths$raw, 'id_map.csv'))
validation_id_map[1:3, ]

```

# More Exploration of Training Gene Expression Data
We should run a linear regression for each cell type. The samples are pseudo-bulk cell type data in each well where the expression of genes are the outcomes and the library, plate and doner are covariates. Let's do this for one cell type.  


The first question: do we have all genes (measured or not measured) per condition? I mean have they recorded zeros? We have two ways of checking this:

Is there any zero value recorded?
```{r , warning = FALSE}
sum(train$count == 0)
```
No! 
Let's check if the total unique genes is present in some random experiment:

```{r , warning = FALSE}
all_genes <- unique(train$gene)
all_samples <- unique(train$obs_id)
length(all_genes)
length(all_samples)
first_cell <- train$obs_id[1]
length(train$gene[train$obs_id == first_cell])
```
The total unique gene names are 21255 and the number of genes recorded in the first cell is 1226. So around 5%. We use sparse matrices to save things per cell type. 

Let's take a look at the count vs. normalized counts:

```{r , warning = FALSE}
summary(train$count[train$obs_id == first_cell])
hist(train$count[train$obs_id == first_cell])


summary(train$normalized_count[train$obs_id == first_cell])
hist(train$normalized_count[train$obs_id == first_cell])
sum(train$normalized_count[train$obs_id == first_cell])

sum(train$normalized_count[train$obs_id == all_samples[100]])
sum(train$count[train$obs_id == all_samples[100]])


sum(train$count[train$obs_id %in% train_meta$obs_id[train_meta$library_id == 'library_4']])

```

```{r , warning = FALSE}
hist(log(train$count[train$obs_id == first_cell]))

```
So it seem that they are doing more than just taking a log. Or is it 2 times the log? It is unclear, there is nothing about that in the code. They just read it from the data file. But the exact normalization method is not important because they pass the count (not normalized) to Limma and limma does the transformation. Per Limma Voom paper the normalizatio is: $\log2(\frac{r_{ig} + 0.5}{R_i + 1} \times 10^6)$ where $r_{ig}$ is the expression of gene $g$ in sample $i$ and $R_i = \sum_g r_{ig}$. We call this Log Count Per Million (CPM). 


I have issues with the concept of library = row in the plate. Because not all rows in different plates are processed the same way. Honestly I don't get it. Let's check the library column too:

```{r , warning = FALSE}
sort(unique(train_meta$library_id))
sort(unique(train_meta$plate_name))
```


OK, now it makes sense: library is not the row id, it is a unique id per row for all rows: 3 * 2 * length(A-H) = 48. 


# Appendix
It's always a good idea to finish up by reporting which versions of the
software tools were used for this analysis:
```{r si}
sessionInfo()
```
